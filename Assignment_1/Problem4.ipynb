{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNBc2FGKRlS1"
      },
      "outputs": [],
      "source": [
        "# This script performs sparse keypoint detection and matching using a pretrained VGG CNN.\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your code goes here"
      ],
      "metadata": {
        "id": "6dIVMPrm3Gw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained VGG Mode\n",
        "model = torchvision.models.vgg16(pretrained=True).eval()\n",
        "cnn_feat_extractor = None ## YOU CHOICE OF INTERMEDIATE LAYER GO HERE ##"
      ],
      "metadata": {
        "id": "a9p6DiYw3Eim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageNet normalization\n",
        "mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "std  = torch.tensor([0.229, 0.224, 0.225])\n",
        "\n",
        "def preprocess_for_cnn(bgr_img):\n",
        "    rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
        "    tensor = torch.from_numpy(rgb).float().permute(2, 0, 1) / 255.0\n",
        "    tensor = (tensor - mean[:, None, None]) / std[:, None, None]\n",
        "    return tensor.unsqueeze(0)  # Add batch dimension"
      ],
      "metadata": {
        "id": "Ayvc0z8rRwyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saliency-based Keypoint Detection\n",
        "def get_keypoints_from_saliency(saliency, max_points=500):\n",
        "    neighborhood = np.ones((3, 3), np.uint8)\n",
        "    local_max = cv2.dilate(saliency.astype(np.float32), neighborhood)\n",
        "    peaks = (saliency == local_max)\n",
        "    coords = np.column_stack(np.nonzero(peaks))\n",
        "    values = saliency[coords[:, 0], coords[:, 1]]\n",
        "    sorted_idx = np.argsort(values)[::-1]\n",
        "    coords = coords[sorted_idx]\n",
        "    if max_points and coords.shape[0] > max_points:\n",
        "        coords = coords[:max_points]\n",
        "    return coords"
      ],
      "metadata": {
        "id": "uTtzSEk_Rxsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Process Images\n",
        "def download_image(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "1zqaJIAlR4hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_images(img1_path, img2_path):\n",
        "    img1 = cv2.imread(img1_path)\n",
        "    img2 = cv2.imread(img2_path)\n",
        "\n",
        "    if img1 is None or img2 is None:\n",
        "        raise FileNotFoundError(\"Could not load one or both images. Check the paths or upload the images.\")\n",
        "\n",
        "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    tensor1 = preprocess_for_cnn(img1)\n",
        "    tensor2 = preprocess_for_cnn(img2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        feat1 = cnn_feat_extractor(tensor1).squeeze(0)  # C x H x W\n",
        "        feat2 = cnn_feat_extractor(tensor2).squeeze(0)\n",
        "\n",
        "    sal1 = torch.norm(feat1, p=2, dim=0).cpu().numpy()\n",
        "    sal2 = torch.norm(feat2, p=2, dim=0).cpu().numpy()\n",
        "\n",
        "    coords1 = get_keypoints_from_saliency(sal1, max_points=500)\n",
        "    coords2 = get_keypoints_from_saliency(sal2, max_points=500)\n",
        "\n",
        "    H1, W1 = sal1.shape\n",
        "    H2, W2 = sal2.shape\n",
        "    keypoints1 = [(float(c * img1.shape[1] / W1), float(r * img1.shape[0] / H1)) for r, c in coords1]\n",
        "    keypoints2 = [(float(c * img2.shape[1] / W2), float(r * img2.shape[0] / H2)) for r, c in coords2]\n",
        "\n",
        "    descriptors1 = [feat1[:, r, c] / (feat1[:, r, c].norm() + 1e-8) for r, c in coords1]\n",
        "    descriptors2 = [feat2[:, r, c] / (feat2[:, r, c].norm() + 1e-8) for r, c in coords2]\n",
        "\n",
        "    desc1_np = torch.stack(descriptors1).cpu().numpy().astype(np.float32)\n",
        "    desc2_np = torch.stack(descriptors2).cpu().numpy().astype(np.float32)\n",
        "\n",
        "    return img1, img2, img1_gray, img2_gray, keypoints1, keypoints2, desc1_np, desc2_np"
      ],
      "metadata": {
        "id": "Vud-9qQiR9rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptor Matching\n",
        "def match_descriptors(desc1, desc2, ratio_thresh=None):\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
        "    knn_matches = bf.knnMatch(desc1, desc2, k=2)\n",
        "    good_matches = [m for m, n in knn_matches if m.distance < ratio_thresh * n.distance]\n",
        "    return good_matches"
      ],
      "metadata": {
        "id": "1j8LiRqISA5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Matches\n",
        "def visualize_matches(img1, img2, kp1, kp2, matches, max_draw=30):\n",
        "    kp_cv1 = [cv2.KeyPoint(float(x), float(y), 5) for (x, y) in kp1]\n",
        "    kp_cv2 = [cv2.KeyPoint(float(x), float(y), 5) for (x, y) in kp2]\n",
        "    img_matches = cv2.drawMatches(img1, kp_cv1, img2, kp_cv2, matches[:max_draw], None,\n",
        "                                  flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Top {max_draw} Deep Feature Matches')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SnF3XaqdSDKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your code goes here"
      ],
      "metadata": {
        "id": "cT_ZOOrp3ixA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Function\n",
        "if __name__ == '__main__':\n",
        "    # Automatically download sample images\n",
        "    download_image(\"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/box.png\", \"image1.jpg\")\n",
        "    download_image(\"https://raw.githubusercontent.com/opencv/opencv/master/samples/data/box_in_scene.png\", \"image2.jpg\")\n",
        "\n",
        "    img1_path = 'image1.jpg'\n",
        "    img2_path = 'image2.jpg'\n",
        "\n",
        "    ratio_thresh = 0.0 ## YOUR VALUES FOR RATIO GO HERE\n",
        "\n",
        "    img1, img2, gray1, gray2, kp1, kp2, desc1, desc2 = load_and_process_images(img1_path, img2_path)\n",
        "    matches = match_descriptors(desc1, desc2)\n",
        "    print(f\"Found {len(matches)} good matches\")\n",
        "    visualize_matches(img1, img2, kp1, kp2, matches)"
      ],
      "metadata": {
        "id": "OPvrE21KSFwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}